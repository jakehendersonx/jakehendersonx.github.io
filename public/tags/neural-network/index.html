<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Neural Network | None() to Some()</title>
<meta name=keywords content><meta name=description content="a tech implementation blog by Jake Henderson"><meta name=author content="Jake Henderson"><link rel=canonical href=http://localhost:1313/tags/neural-network/><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=http://localhost:1313/tags/neural-network/index.xml><link rel=alternate hreflang=en href=http://localhost:1313/tags/neural-network/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="http://localhost:1313/tags/neural-network/"><meta property="og:site_name" content="None() to Some()"><meta property="og:title" content="Neural Network"><meta property="og:description" content="a tech implementation blog by Jake Henderson"><meta property="og:locale" content="en"><meta property="og:type" content="website"><meta property="og:image" content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Neural Network"><meta name=twitter:description content="a tech implementation blog by Jake Henderson"></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Home (Alt + H)"><img src=http://localhost:1313/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/posts/ title=posts><span>posts</span></a></li><li><a href=http://localhost:1313/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/tags/>Tags</a></div><h1>Neural Network
<a href=/tags/neural-network/index.xml title=RSS aria-label=RSS><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" height="23"><path d="M4 11a9 9 0 019 9"/><path d="M4 4a16 16 0 0116 16"/><circle cx="5" cy="19" r="1"/></svg></a></h1></header><article class="post-entry tag-entry"><figure class=entry-cover><img loading=lazy src=http://localhost:1313/%3Cimage%20path/url%3E alt></figure><header class=entry-header><h2 class=entry-hint-parent>Neural Network Intuition: A Neural Network that Adds Two Numbers in PyTorch</h2></header><div class=entry-content><p>Outline brief intro of why do problems like this to test intuition link to project code a neural net approximates this equation simple addition of 2 numers approximates a simpler equation that is now just about converging the x weights and error what does normalization do go over MSE loss and Adam Optimizer -> link to resources generate the data and train the neural networks (mine, claude, and ) What Function are we Approximating Defining AddNet 1 2 3 4 5 6 7 8 9 10 11 12 13 import torch import torch.nn as nn import numpy as np class AddNet(nn.Module): def __init__(self): super(AddNet, self).__init__() #TODO: explain simple linear equation facilitated self.output = nn.Linear(2, 1) def forward(self, x): return self.output(x) Normalization and Data Generation 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def normalize(x): """Normalizes input values to range [0,1].""" return x / 100.0 def denormalize(x): """Denormalizes values back to original range.""" return x * 100.0 def generate_training_data(num_samples): x1 = torch.randint(0, 101, (num_samples, 1), dtype=torch.float32) x2 = torch.randint(0, 101, (num_samples, 1), dtype=torch.float32) inputs = torch.cat((normalize(x1), normalize(x2)), dim=1) targets = normalize(x1 + x2) return inputs, targets Loss Function, Optimizer, and 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def train_add_net(model, num_epochs=500, batch_size=32): criterion = nn.MSELoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.05) inputs, targets = generate_training_data(1000) for epoch in range(num_epochs): idx = torch.randperm(inputs.shape[0])[:batch_size] batch_inputs = inputs[idx] batch_targets = targets[idx] outputs = model(batch_inputs) loss = criterion(outputs, batch_targets) optimizer.zero_grad() loss.backward() optimizer.step() if (epoch + 1) % 100 == 0: print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}') print_weights_and_bias(model) Inference 1 2 3 4 5 6 7 8 9 10 11 def inference(model, x1, x2): model.eval() with torch.no_grad(): x1 = max(0, min(100, int(x1))) x2 = max(0, min(100, int(x2))) x1_norm = normalize(torch.tensor([[float(x1)]], dtype=torch.float32)) x2_norm = normalize(torch.tensor([[float(x2)]], dtype=torch.float32)) inputs = torch.cat((x1_norm, x2_norm), dim=1) predicted_sum_norm = model(inputs) return denormalize(predicted_sum_norm).item() Results 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def batch_test(model, num_samples): print(f"\nBatch Testing ({num_samples} samples):") print("-" * 50) total_percent_error = 0 for _ in range(num_samples): x1 = np.random.randint(0, 101) x2 = np.random.randint(0, 101) predicted_sum = test_addition(model, x1, x2) actual_sum = x1 + x2 percent_error = abs(predicted_sum - actual_sum) / actual_sum * 100 total_percent_error += percent_error average_percent_error = total_percent_error / num_samples print(f"\nAverage Percent Error across {num_samples} samples: {average_percent_error:.2f}%") Flagship AI Model’s Responce to the prompt 1 2 3 4 5 Write a neural network in PyTorch that can add two numbers. Call it AddNet. It should add two whole numbers that are between 0 and 100. Goal 1 is to get it to add two arbitrary whole numbers in a defined range with the lowest possible error. Goal 2 is to keep the neural net and training definitions to be as simple as possible while still accomplishing Goal 1. At each relevant step of your network definition and training, write about why you made the choice to use that "technique". ChatGPT-4o’s Atttempt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 import torch import torch.nn as nn import torch.optim as optim import numpy as np # Define the range of input numbers NUM_RANGE = 101 # From 0 to 100 inclusive # Define the AddNet model class AddNet(nn.Module): def __init__(self): super(AddNet, self).__init__() # Simple feedforward network with 1 hidden layer self.fc1 = nn.Linear(2, 10) # 2 inputs (two numbers) -> 10 hidden neurons self.fc2 = nn.Linear(10, 1) # 10 hidden neurons -> 1 output (sum) def forward(self, x): x = torch.relu(self.fc1(x)) # ReLU activation for non-linearity x = self.fc2(x) # No activation for output layer (regression task) return x # Step 1: Initialize the model, loss function, and optimizer model = AddNet() criterion = nn.MSELoss() # Mean Squared Error Loss for regression optimizer = optim.Adam(model.parameters(), lr=0.01) # Adam optimizer for fast convergence # Explain choice of architecture: # - Input: Two numbers as a 2D tensor (e.g., [45, 30]) # - Hidden layer: A small fully connected layer with 10 neurons balances simplicity and capacity # - ReLU: Adds non-linearity to allow the network to learn complex relationships # - Output: Single value (sum), no activation because this is a regression problem # Step 2: Generate training data def generate_data(size): x_data = np.random.randint(0, NUM_RANGE, size=(size, 2)) # Random pairs of integers y_data = np.sum(x_data, axis=1, keepdims=True) # Compute sums return torch.tensor(x_data, dtype=torch.float32), torch.tensor(y_data, dtype=torch.float32) train_x, train_y = generate_data(10000) # Generate 10,000 training examples # Step 3: Train the model epochs = 500 for epoch in range(epochs): optimizer.zero_grad() # Clear gradients predictions = model(train_x) # Forward pass loss = criterion(predictions, train_y) # Compute loss loss.backward() # Backward pass optimizer.step() # Update weights # Print progress if (epoch + 1) % 50 == 0: print(f"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}") # Explain training process: # - Loss: MSE is suitable for regression because it penalizes large errors # - Optimizer: Adam is chosen for its adaptive learning rate and robustness # - Epochs: 500 iterations are enough to converge for this simple task # - Data: 10,000 examples provide sufficient coverage of input space (0 to 100) # Step 4: Test the model test_x, test_y = generate_data(1000) # Generate test data with torch.no_grad(): # No gradient computation during testing test_predictions = model(test_x) test_loss = criterion(test_predictions, test_y) print(f"Test Loss: {test_loss.item():.4f}") # Step 5: Demonstrate prediction example = torch.tensor([[23, 45]], dtype=torch.float32) with torch.no_grad(): prediction = model(example).item() print(f"Prediction for input {example.tolist()}: {prediction:.2f} (Expected: {sum(example.tolist()[0])})") Claude 3.5 Sonnet’s Attempt 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 import torch import torch.nn as nn import torch.optim as optim import numpy as np class AddNet(nn.Module): def __init__(self): super(AddNet, self).__init__() # A simple architecture with two hidden layers self.network = nn.Sequential( nn.Linear(2, 8), nn.ReLU(), nn.Linear(8, 4), nn.ReLU(), nn.Linear(4, 1) ) def forward(self, x): return self.network(x) # Initialize the model model = AddNet() criterion = nn.MSELoss() optimizer = optim.Adam(model.parameters(), lr=0.01) # Generate training data def generate_data(num_samples=1000): # Generate random numbers between -10 and 10 x1 = np.random.uniform(-10, 10, (num_samples, 1)) x2 = np.random.uniform(-10, 10, (num_samples, 1)) # True sums y = x1 + x2 # Convert to PyTorch tensors X = torch.FloatTensor(np.hstack((x1, x2))) y = torch.FloatTensor(y) return X, y # Training loop def train(epochs=1000): for epoch in range(epochs): X, y = generate_data() # Forward pass optimizer.zero_grad() output = model(X) loss = criterion(output, y) # Backward pass and optimize loss.backward() optimizer.step() if (epoch + 1) % 100 == 0: print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.6f}') # Train the model train() # Test the model def test_addition(x1, x2): model.eval() with torch.no_grad(): input_tensor = torch.FloatTensor([[x1, x2]]) prediction = model(input_tensor) return prediction.item() # Example usage x1, x2 = 5.7, 3.2 predicted_sum = test_addition(x1, x2) actual_sum = x1 + x2 print(f"\nTest Result:") print(f"Numbers: {x1} + {x2}") print(f"Predicted sum: {predicted_sum:.4f}") print(f"Actual sum: {actual_sum:.4f}") print(f"Error: {abs(predicted_sum - actual_sum):.4f}")</p></div><footer class=entry-footer><span title='2020-09-15 11:30:03 +0000 +0000'>September 15, 2020</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;1247 words&nbsp;·&nbsp;Jake Henderson</footer><a class=entry-link aria-label="post link to Neural Network Intuition: A Neural Network that Adds Two Numbers in PyTorch" href=http://localhost:1313/posts/add_numbers/></a></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>None() to Some()</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>